# -*- coding: utf-8 -*-
"""NewModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iNjakptndCHBrfGw4d0Iec9mPehI8VnO
"""

# Impor library yang diperlukan
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy.stats import mode
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Memuat dan Mempersiapkan Data ---
# Gunakan file CSV baru Anda dengan label 0-25
file_path = 'Data.csv'
# Specify the semicolon delimiter
df = pd.read_csv(file_path, delimiter=';')

print("Contoh Data:")
print(df.head())
print("\nInfo Data:")
df.info()
print("\nJumlah Sampel untuk Setiap Kelas:")
print(df['ClassID'].value_counts().sort_index())

# Pisahkan fitur (X) dan label (y)
X = df.drop('ClassID', axis=1).values
y = df['ClassID'].values

# Tentukan kolom fitur untuk digunakan saat menyimpan nilai normalisasi
features_columns = df.drop('ClassID', axis=1).columns.tolist()

# --- 2. Normalisasi Fitur ---
# Pisahkan data menjadi set training dan testing/validasi
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Buat dan fit StandardScaler HANYA pada data training
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Simpan nilai means dan standard deviations untuk digunakan di ESP32
means = scaler.mean_
std_devs = scaler.scale_

print("\n--- Nilai untuk Normalisasi di ESP32 ---")
print("const float MEANS[] = {")
print(", ".join([f"{val:.4f}" for val in means]) + "};")
print("const float STD_DEVS[] = {")
print(", ".join([f"{val:.4f}" for val in std_devs]) + "};")
print("Urutan Fitur:", features_columns)
print("----------------------------------------\n")

# --- 3. [Opsional] Windowing dan Flattening (jika Anda ingin menangkap pola waktu) ---
X_train_final = X_train_scaled
y_train_final = y_train
X_test_final = X_test_scaled
y_test_final = y_test

NUM_FEATURES = X_train_final.shape[1]
NUM_CLASSES = len(np.unique(y))

# --- 4. Membangun Model ANN yang Ditingkatkan ---
def create_enhanced_ann_model(num_features, num_classes):
    model = Sequential([
        Input(shape=(num_features,), name='input_layer'),

        Dense(128, kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        tf.keras.layers.ReLU(),
        Dropout(0.4),

        Dense(64, kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        tf.keras.layers.ReLU(),
        Dropout(0.3),

        Dense(64, kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        tf.keras.layers.ReLU(),
        Dropout(0.2),

        Dense(num_classes, activation='softmax', name='output_layer')
    ])
    return model

# Buat model
model = create_enhanced_ann_model(NUM_FEATURES, NUM_CLASSES)

# Kompilasi model dengan learning rate yang mungkin sedikit lebih rendah
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Generate and save the model plot
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)

# Display the generated image in the notebook
from IPython.display import Image
display(Image(filename='model.png', width=400, height=1800)) # Adjusted width and height

# --- 5. Melatih Model ---
# Callbacks untuk menyimpan model terbaik dan menghentikan training lebih awal
model_checkpoint = ModelCheckpoint('sibi_ann_model_enhanced.keras', save_best_only=True, monitor='val_accuracy', mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train_final,
    y_train_final,
    epochs=25,
    batch_size=32,
    validation_data=(X_test_final, y_test_final),
    callbacks=[model_checkpoint, early_stopping]
)

# --- 6. Evaluasi Model ---
# Muat model terbaik yang disimpan
best_model = tf.keras.models.load_model('sibi_ann_model_enhanced.keras')

# Evaluasi pada data test
test_loss, test_accuracy = best_model.evaluate(X_test_final, y_test_final)
print(f"\nAkurasi pada data test: {test_accuracy*100:.2f}%")

# Plot hasil training
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5)) # Sedikit diperbesar untuk kejelasan

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.grid(True)


plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.grid(True)

plt.tight_layout()
plt.show()

# --- 7. Prediksi dan Analisis Tambahan (Optional) ---
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Get predictions for the test set
y_pred_probs = best_model.predict(X_test_final)
y_pred = np.argmax(y_pred_probs, axis=1)

# --- Confusion Matrix ---
print("\n--- Confusion Matrix ---")
cm = confusion_matrix(y_test_final, y_pred)

# Define class_names based on the unique values in the original 'ClassID' column
# Assuming 'df' contains the original data and 'ClassID' is the column with class labels
if 'df' in locals() and 'ClassID' in df.columns:
    class_names = sorted(df['ClassID'].unique())
else:
    # Fallback if df or 'ClassID' is not available, though it should be based on the notebook state
    # In this case, we'll just create generic class names
    class_names = [str(i) for i in range(NUM_CLASSES)] # Use NUM_CLASSES defined earlier


# Plot confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# --- Classification Report ---
print("\n--- Classification Report ---")
# Generate class labels based on the unique classes in y_test_final
# Assuming y_test_final contains the actual class indices (0-25)
class_labels = [str(i) for i in sorted(np.unique(y_test_final))]
print(classification_report(y_test_final, y_pred, target_names=class_labels))

# Tampilkan beberapa prediksi vs label asli
print("\n--- Sample Predictions ---")
for i in range(10): # Tampilkan 10 sampel pertama
    # Access y_test using positional index .iloc[i]
    print(f"Sample {i+1}: Actual ClassID = {y_test_final[i]+1}, Predicted ClassID = {y_pred[i]+1}")

!pip install "everywhereml>=0.2.32"

"""
Export NN to C++
Copy-paste the generated code inside a file named model.h or irisModel.h
in your Arduino project
"""
from everywhereml.code_generators.tensorflow import convert_model
import tensorflow as tf # Import tensorflow for one-hot encoding

# Use the normalized training data as the representative dataset for C++ export
# convert_model needs input data to understand feature scaling/ranges for C++ generation
X_np = X_test_final # Changed from X.values to X_train_normalized

# One-hot encode the target variable for the C++ conversion
# Assuming NUM_CLASSES is the total number of classes
y_train_one_hot = tf.keras.utils.to_categorical(y_test_final, num_classes=26)

# Use the loaded best model (best_model_ann_loaded) for conversion
# Pass the one-hot encoded y to the convert_model function
c_header = convert_model(best_model, X_np, y_train_one_hot, model_name='TA_Model')
print(c_header)

with open('MODEL.h', 'w') as f:
        f.write(c_header)